{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "data = ''\n",
    "\n",
    "# opening each file and appending to data\n",
    "# files = os.listdir(path)\n",
    "# for file in files:\n",
    "#     if os.path.isfile(os.path.join(path, file)):\n",
    "#         file_content = open(os.path.join(path, file), 'r', encoding='utf-8').read()\n",
    "#         data += file_content\n",
    "\n",
    "files = ['bieber', 'bruno-mars', 'drake', 'rihanna', 'adele']\n",
    "for file in files:\n",
    "    if os.path.isfile(os.path.join(path, file + '.txt')):\n",
    "        file_content = open(os.path.join(path, file + '.txt'), 'r', encoding='utf-8').read()\n",
    "        data += file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus: 711225\n",
      "What do you mean?\n",
      "Oh, oh, oh\n",
      "When you sometimes say yes\n",
      "But you sometimes say no\n",
      "What do you mean?\n",
      "Hey, yeah\n",
      "When you don't want me to move\n",
      "But you tell me to go\n",
      "What do you mean?\n",
      "Oh\n",
      "What do you mean?\n"
     ]
    }
   ],
   "source": [
    "print('Length of corpus:', len(data))\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters before: 90\n",
      "Number of unique characters after: 82\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique characters before:', len(set(data)))\n",
    "\n",
    "# Replace all non ascii characters in data with ''\n",
    "data = re.sub(r'[^\\x00-\\x7F]', r'', data)\n",
    "print('Number of unique characters after:', len(set(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 82\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(data))\n",
    "print('Total chars:', len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 236991\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(data) - maxlen, step):\n",
    "    sentences.append(data[i: i + maxlen])\n",
    "    next_chars.append(data[i + maxlen])\n",
    "\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype = np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype = np.bool)\n",
    "for i, sent in enumerate(sentences):\n",
    "    for t, char in enumerate(sent):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 40, 128)           108032    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 82)                10578     \n",
      "=================================================================\n",
      "Total params: 250,194\n",
      "Trainable params: 250,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    InputLayer(input_shape = (maxlen, len(chars))),\n",
    "    LSTM(units = 128, activation = 'tanh', return_sequences = True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units = 128, activation = 'tanh'),\n",
    "    Dense(units = len(chars), activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(learning_rate = 1e-3, decay = 1e-5)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1852/1852 [==============================] - 35s 15ms/step - loss: 2.4415\n",
      "Epoch 2/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 1.9205\n",
      "Epoch 3/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.7345\n",
      "Epoch 4/200\n",
      "1852/1852 [==============================] - 29s 15ms/step - loss: 1.6245\n",
      "Epoch 5/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.5518\n",
      "Epoch 6/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.4930\n",
      "Epoch 7/200\n",
      "1852/1852 [==============================] - 31s 17ms/step - loss: 1.4478\n",
      "Epoch 8/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.4106\n",
      "Epoch 9/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.3769\n",
      "Epoch 10/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.3473\n",
      "Epoch 11/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.3220\n",
      "Epoch 12/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.2982\n",
      "Epoch 13/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 1.2771\n",
      "Epoch 14/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.2575\n",
      "Epoch 15/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 1.2401\n",
      "Epoch 16/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 1.2225\n",
      "Epoch 17/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.2078\n",
      "Epoch 18/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.1914\n",
      "Epoch 19/200\n",
      "1852/1852 [==============================] - 29s 15ms/step - loss: 1.1774\n",
      "Epoch 20/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.1646\n",
      "Epoch 21/200\n",
      "1852/1852 [==============================] - 29s 15ms/step - loss: 1.1532\n",
      "Epoch 22/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.1420\n",
      "Epoch 23/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.1313\n",
      "Epoch 24/200\n",
      "1852/1852 [==============================] - 29s 15ms/step - loss: 1.1203\n",
      "Epoch 25/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.1111\n",
      "Epoch 26/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.0990\n",
      "Epoch 27/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.0904\n",
      "Epoch 28/200\n",
      "1852/1852 [==============================] - 31s 16ms/step - loss: 1.0812\n",
      "Epoch 29/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 1.0736\n",
      "Epoch 30/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.0645\n",
      "Epoch 31/200\n",
      "1852/1852 [==============================] - 33s 18ms/step - loss: 1.0570\n",
      "Epoch 32/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.0495\n",
      "Epoch 33/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.0401\n",
      "Epoch 34/200\n",
      "1852/1852 [==============================] - 38s 21ms/step - loss: 1.0356\n",
      "Epoch 35/200\n",
      "1852/1852 [==============================] - 31s 17ms/step - loss: 1.0276\n",
      "Epoch 36/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.0197\n",
      "Epoch 37/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.0128\n",
      "Epoch 38/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 1.0083\n",
      "Epoch 39/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 1.0004\n",
      "Epoch 40/200\n",
      "1852/1852 [==============================] - 34s 18ms/step - loss: 0.9954\n",
      "Epoch 41/200\n",
      "1852/1852 [==============================] - 34s 19ms/step - loss: 0.9911\n",
      "Epoch 42/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.9836\n",
      "Epoch 43/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9794\n",
      "Epoch 44/200\n",
      "1852/1852 [==============================] - 31s 17ms/step - loss: 0.9740\n",
      "Epoch 45/200\n",
      "1852/1852 [==============================] - 37s 20ms/step - loss: 0.9687\n",
      "Epoch 46/200\n",
      "1852/1852 [==============================] - 35s 19ms/step - loss: 0.9628\n",
      "Epoch 47/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.9591\n",
      "Epoch 48/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9549\n",
      "Epoch 49/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9474\n",
      "Epoch 50/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9452\n",
      "Epoch 51/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.9405\n",
      "Epoch 52/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9331\n",
      "Epoch 53/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9297\n",
      "Epoch 54/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9268\n",
      "Epoch 55/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9224\n",
      "Epoch 56/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9189\n",
      "Epoch 57/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9151\n",
      "Epoch 58/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 0.9111\n",
      "Epoch 59/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9064\n",
      "Epoch 60/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.9037\n",
      "Epoch 61/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8997\n",
      "Epoch 62/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.8981\n",
      "Epoch 63/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8922\n",
      "Epoch 64/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8886\n",
      "Epoch 65/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8852\n",
      "Epoch 66/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8828\n",
      "Epoch 67/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8782\n",
      "Epoch 68/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8748\n",
      "Epoch 69/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8716\n",
      "Epoch 70/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8699\n",
      "Epoch 71/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8651\n",
      "Epoch 72/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8625\n",
      "Epoch 73/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8603\n",
      "Epoch 74/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8574\n",
      "Epoch 75/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.8548\n",
      "Epoch 76/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8528\n",
      "Epoch 77/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8512\n",
      "Epoch 78/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8454\n",
      "Epoch 79/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8431\n",
      "Epoch 80/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8425\n",
      "Epoch 81/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8381\n",
      "Epoch 82/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8344\n",
      "Epoch 83/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8313\n",
      "Epoch 84/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8309\n",
      "Epoch 85/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8281\n",
      "Epoch 86/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8232\n",
      "Epoch 87/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8236\n",
      "Epoch 88/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8164\n",
      "Epoch 89/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8198\n",
      "Epoch 90/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8135\n",
      "Epoch 91/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8118\n",
      "Epoch 92/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8105\n",
      "Epoch 93/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8077\n",
      "Epoch 94/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8050\n",
      "Epoch 95/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8041\n",
      "Epoch 96/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8024\n",
      "Epoch 97/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.8012\n",
      "Epoch 98/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7974\n",
      "Epoch 99/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7972\n",
      "Epoch 100/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7949\n",
      "Epoch 101/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7911\n",
      "Epoch 102/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7879\n",
      "Epoch 103/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7882\n",
      "Epoch 104/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7841\n",
      "Epoch 105/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7838\n",
      "Epoch 106/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7818\n",
      "Epoch 107/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7817\n",
      "Epoch 108/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 0.7782\n",
      "Epoch 109/200\n",
      "1852/1852 [==============================] - 29s 15ms/step - loss: 0.7762\n",
      "Epoch 110/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7742\n",
      "Epoch 111/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7706\n",
      "Epoch 112/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7712\n",
      "Epoch 113/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7684\n",
      "Epoch 114/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7669\n",
      "Epoch 115/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7650\n",
      "Epoch 116/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7614\n",
      "Epoch 117/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7593\n",
      "Epoch 118/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7581\n",
      "Epoch 119/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7582\n",
      "Epoch 120/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7569\n",
      "Epoch 121/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7531\n",
      "Epoch 122/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7517\n",
      "Epoch 123/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7498\n",
      "Epoch 124/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7497\n",
      "Epoch 125/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7488\n",
      "Epoch 126/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7464\n",
      "Epoch 127/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 0.7469\n",
      "Epoch 128/200\n",
      "1852/1852 [==============================] - 26s 14ms/step - loss: 0.7440\n",
      "Epoch 129/200\n",
      "1852/1852 [==============================] - 26s 14ms/step - loss: 0.7421\n",
      "Epoch 130/200\n",
      "1852/1852 [==============================] - 26s 14ms/step - loss: 0.7385\n",
      "Epoch 131/200\n",
      "1852/1852 [==============================] - 26s 14ms/step - loss: 0.7389\n",
      "Epoch 132/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7365\n",
      "Epoch 133/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7360\n",
      "Epoch 134/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7328\n",
      "Epoch 135/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7351\n",
      "Epoch 136/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7292\n",
      "Epoch 137/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7272\n",
      "Epoch 138/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7239\n",
      "Epoch 139/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7260\n",
      "Epoch 140/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7240\n",
      "Epoch 141/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7233\n",
      "Epoch 142/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7233\n",
      "Epoch 143/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 0.7237\n",
      "Epoch 144/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.7199\n",
      "Epoch 145/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 0.7169\n",
      "Epoch 146/200\n",
      "1852/1852 [==============================] - 29s 15ms/step - loss: 0.7155\n",
      "Epoch 147/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7161\n",
      "Epoch 148/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7130\n",
      "Epoch 149/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7128\n",
      "Epoch 150/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7118\n",
      "Epoch 151/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7095\n",
      "Epoch 152/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7074\n",
      "Epoch 153/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7067\n",
      "Epoch 154/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7080\n",
      "Epoch 155/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.7059\n",
      "Epoch 156/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.7001\n",
      "Epoch 157/200\n",
      "1852/1852 [==============================] - 32s 17ms/step - loss: 0.7012\n",
      "Epoch 158/200\n",
      "1852/1852 [==============================] - 45s 24ms/step - loss: 0.7013\n",
      "Epoch 159/200\n",
      "1852/1852 [==============================] - 46s 25ms/step - loss: 0.7008\n",
      "Epoch 160/200\n",
      "1852/1852 [==============================] - 47s 25ms/step - loss: 0.6996\n",
      "Epoch 161/200\n",
      "1852/1852 [==============================] - 48s 26ms/step - loss: 0.6977\n",
      "Epoch 162/200\n",
      "1852/1852 [==============================] - 39s 21ms/step - loss: 0.6958\n",
      "Epoch 163/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.6953\n",
      "Epoch 164/200\n",
      "1852/1852 [==============================] - 36s 19ms/step - loss: 0.6924\n",
      "Epoch 165/200\n",
      "1852/1852 [==============================] - 36s 19ms/step - loss: 0.6949\n",
      "Epoch 166/200\n",
      "1852/1852 [==============================] - 32s 17ms/step - loss: 0.6926\n",
      "Epoch 167/200\n",
      "1852/1852 [==============================] - 31s 17ms/step - loss: 0.6902\n",
      "Epoch 168/200\n",
      "1852/1852 [==============================] - 31s 17ms/step - loss: 0.6911\n",
      "Epoch 169/200\n",
      "1852/1852 [==============================] - 31s 17ms/step - loss: 0.6886\n",
      "Epoch 170/200\n",
      "1852/1852 [==============================] - 31s 17ms/step - loss: 0.6871\n",
      "Epoch 171/200\n",
      "1852/1852 [==============================] - 33s 18ms/step - loss: 0.6863\n",
      "Epoch 172/200\n",
      "1852/1852 [==============================] - 33s 18ms/step - loss: 0.6838\n",
      "Epoch 173/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6872\n",
      "Epoch 174/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6817\n",
      "Epoch 175/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.6830\n",
      "Epoch 176/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.6821\n",
      "Epoch 177/200\n",
      "1852/1852 [==============================] - 29s 15ms/step - loss: 0.6817\n",
      "Epoch 178/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.6772\n",
      "Epoch 179/200\n",
      "1852/1852 [==============================] - 30s 16ms/step - loss: 0.6766\n",
      "Epoch 180/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.6778\n",
      "Epoch 181/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.6771\n",
      "Epoch 182/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.6751\n",
      "Epoch 183/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.6725\n",
      "Epoch 184/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.6723\n",
      "Epoch 185/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.6716\n",
      "Epoch 186/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.6709\n",
      "Epoch 187/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.6695\n",
      "Epoch 188/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6702\n",
      "Epoch 189/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.6683\n",
      "Epoch 190/200\n",
      "1852/1852 [==============================] - 27s 15ms/step - loss: 0.6672\n",
      "Epoch 191/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6653\n",
      "Epoch 192/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6653\n",
      "Epoch 193/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6635\n",
      "Epoch 194/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6632\n",
      "Epoch 195/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6620\n",
      "Epoch 196/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6608\n",
      "Epoch 197/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6609\n",
      "Epoch 198/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6593\n",
      "Epoch 199/200\n",
      "1852/1852 [==============================] - 28s 15ms/step - loss: 0.6584\n",
      "Epoch 200/200\n",
      "1852/1852 [==============================] - 29s 16ms/step - loss: 0.6580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1664451c670>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 128\n",
    "\n",
    "callback = ModelCheckpoint(filepath = 'checkpoints/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor = 'loss', save_best_only = True, mode = 'min')\n",
    "model.fit(x, y, batch_size = batch_size, epochs = epochs, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with seed: \"ver sinkin'\n",
      "'Bout to set it off, in this\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanket\\AppData\\Local\\Temp/ipykernel_13016/1116348211.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " charch\n",
      "This feeling up\n",
      "All the girl along\n",
      "See up to purply lights On the mistow\n",
      "Why with you to sind me\n",
      "I wanna make o long off\n",
      "Nobody that happen me to see What do you mean\n",
      "more you smece\n",
      "I feel you thinks I know How I would rew a Lour bust into right\n",
      "You got a feelings and your heart\n",
      "Whore she's the only one (girl better wantine)\n",
      "Sayes you love me at the frink with the damb\n",
      "Shownlies where you \n"
     ]
    }
   ],
   "source": [
    "start_index = random.randint(0, len(data) - maxlen - 1)\n",
    "\n",
    "generated = \"\"\n",
    "sentence = data[start_index : start_index + maxlen]\n",
    "print('Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "for i in range(400):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.0\n",
    "        \n",
    "    preds = model.predict(x_pred)[0]\n",
    "\n",
    "    next_index = sample(preds)\n",
    "    next_char = indices_char[next_index]\n",
    "    sentence = sentence[1:] + next_char\n",
    "    generated += next_char\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_200\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_200\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
